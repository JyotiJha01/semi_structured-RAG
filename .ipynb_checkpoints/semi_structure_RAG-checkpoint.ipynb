{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5bc6688-adeb-4262-9508-63201ced2ebb",
   "metadata": {},
   "source": [
    "# Semi Structured RAG\n",
    "\n",
    "Many documents contain a mixture of content types, including text and tables.\n",
    "\n",
    "Semi-structured data can be challenging for conventional RAG for at least two reasons:\n",
    "\n",
    "   - Text splitting may break up tables, corrupting the data in retrieval\n",
    "   - Embedding tables may pose challenges for semantic similarity search\n",
    "\n",
    "This **Notebook** shows how to perform RAG on documents with semi-structured data:\n",
    "\n",
    "-  We will use Unstructured to parse both text and tables from documents (PDFs).\n",
    "- We will use the multi-vector retriever to store raw tables, text along with table summaries better suited for retrieval.\n",
    "- We will use LCEL(LangChain Expression Language) to implement the chains used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c073e6-1257-4e9c-9fb6-616eeae1c875",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ead8d4e-1e29-4100-9b29-0cbd6530cb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install required packages\n",
    "! pip install langchain unstructured[all-docs] langchain_community chromadb pydantic lxml langchainhub -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "485bde95-e95a-4cb3-890f-f31ddc745c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Tesseract and Poppler-utils using apt\n",
    "# !sudo apt install tesseract-ocr \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4b50747-b3b8-4fcc-90aa-81edce391c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tesseract 4.1.1\n",
      " leptonica-1.82.0\n",
      "  libgif 5.1.9 : libjpeg 8d (libjpeg-turbo 2.1.1) : libpng 1.6.37 : libtiff 4.3.0 : zlib 1.2.11 : libwebp 1.2.2 : libopenjp2 2.4.0\n",
      " Found SSE\n",
      " Found libarchive 3.6.0 zlib/1.2.11 liblzma/5.2.5 bz2lib/1.0.8 liblz4/1.9.3 libzstd/1.4.8\n"
     ]
    }
   ],
   "source": [
    "!tesseract --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60506eea-680d-49d4-b91a-e5606618dd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: poppler-utils in /home/jyoti/.local/lib/python3.10/site-packages (0.1.0)\n",
      "Requirement already satisfied: Click>=7.0 in /usr/lib/python3/dist-packages (from poppler-utils) (8.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install poppler-utils\n",
    "!pip install pytesseract -q\n",
    "!pip install nltk -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1273eeea-9d2e-46a7-ba22-87ecf27bd52c",
   "metadata": {},
   "source": [
    "The PDF partitioning used by Unstructured will use:\n",
    "\n",
    "   - tesseract for Optical Character Recognition (OCR)\n",
    "   - poppler for PDF rendering and processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec59923-ebe5-4284-b1b3-75fc58e16dc8",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f205390c-caa4-449e-a620-991d854b9df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "import pytesseract\n",
    "import nltk\n",
    "import nltk.internals\n",
    "# nltk.download('punkt')\n",
    "from typing import Any\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d16ce0-27b0-4df7-bad0-89ba534b5858",
   "metadata": {},
   "source": [
    "## 3. Define Path and Load PDF\n",
    "\n",
    "Partition PDF tables and text\n",
    "\n",
    "Apply to the Gemini paper: https://arxiv.org/abs/2312.11805\n",
    "\n",
    "We use the Unstructured partition_pdf, which segments a PDF document by using a layout model.\n",
    "\n",
    "This layout model makes it possible to extract elements, such as tables, from pdfs.\n",
    "\n",
    "We also can use Unstructured chunking, which:\n",
    "\n",
    " -   Tries to identify document sections (e.g., Introduction, etc)\n",
    "-   Then, builds text blocks that maintain sections while also honoring user-defined chunk sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9f894e1-8063-4dc8-a110-8be41121ecd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF parsed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "path = \"min_gemini.pdf\"  # actual path of PDF\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.isfile(path):\n",
    "    raise FileNotFoundError(f\"The file {path} does not exist.\")\n",
    "\n",
    "# Try to load and parse the PDF\n",
    "try:\n",
    "    # Get elements from PDF\n",
    "    raw_pdf_elements = partition_pdf(\n",
    "        filename=path,\n",
    "        extract_images_in_pdf=False,\n",
    "        infer_table_structure=True,\n",
    "        chunking_strategy=\"by_title\",\n",
    "        max_characters=4000,\n",
    "        new_after_n_chars=3800,\n",
    "        combine_text_under_n_chars=2000,\n",
    "        image_output_dir_path=path,\n",
    "    )\n",
    "    print(\"PDF parsed successfully!\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"File not found: {e}\")\n",
    "except PermissionError as e:\n",
    "    print(f\"Permission denied: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while parsing the PDF: {e}\")\n",
    "    print(\"Please check if the PDF file is corrupted or if it's password-protected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ad4381-b6a8-4349-94d0-7ddf355a5f51",
   "metadata": {},
   "source": [
    "## 4. Examine Extracted Elements\n",
    "\n",
    "\n",
    "\n",
    "We can examine the elements extracted by partition_pdf.\n",
    "\n",
    "CompositeElement are aggregated chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b2602d5-a43c-4e2c-b991-58f5193da3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"<class 'unstructured.documents.elements.CompositeElement'>\": 12, \"<class 'unstructured.documents.elements.Table'>\": 4}\n"
     ]
    }
   ],
   "source": [
    "category_counts = {}\n",
    "\n",
    "for element in raw_pdf_elements:\n",
    "    category = str(type(element))\n",
    "    if category in category_counts:\n",
    "        category_counts[category] += 1\n",
    "    else:\n",
    "        category_counts[category] = 1\n",
    "\n",
    "unique_categories = set(category_counts.keys())\n",
    "print(category_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44d55171-92db-444c-ba0b-c4a7ffc009b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "class Element(BaseModel):\n",
    "    type: str\n",
    "    text: Any\n",
    "\n",
    "categorized_elements = []\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"table\", text=str(element)))\n",
    "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"text\", text=str(element)))\n",
    "\n",
    "table_elements = [e for e in categorized_elements if e.type == \"table\"]\n",
    "print(len(table_elements))\n",
    "\n",
    "text_elements = [e for e in categorized_elements if e.type == \"text\"]\n",
    "print(len(text_elements))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a87dca56-d726-47b5-854e-969dbfcd63cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "597f327c-93ad-4a30-9469-c626bc4ac258",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34beabd0-0f94-4dcb-bcc2-77cd89ba5dfc",
   "metadata": {},
   "source": [
    "## 5. Summarize Tables and Text\n",
    "\n",
    "Use *multi-vector-retriever* to produce summaries of tables and, optionally, text.\n",
    "\n",
    "With the summary, we will also store the raw table elements.\n",
    "\n",
    "The summaries are used to improve the quality of retrieval, as explained in the multi vector retriever docs.\n",
    "\n",
    "The raw tables are passed to the LLM, providing the full table context for the LLM to generate the answer.\n",
    "\n",
    "### **Summarie**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f21197a2-ba35-4e9a-b7f9-06826666b284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "obj = hub.pull(\"rlm/multi-vector-retriever-summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d7587b4-a879-4e51-b431-bafae2e887e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. \\\n",
    "Give a concise summary of the table or text. Table or text chunk: {element} \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# Summary chain\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0525d2c6-1aa7-4185-aa40-e9bcb30c34d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to tables\n",
    "tables = [i.text for i in table_elements]\n",
    "table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cac0c37-d1a3-4d71-ae41-258bcb2af3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to texts\n",
    "texts = [i.text for i in text_elements]\n",
    "text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a59c967-c8d1-4de2-a877-677a7e0b0ce1",
   "metadata": {},
   "source": [
    "## 6. Add to Vector Store\n",
    "\n",
    "Use Multi Vector Retriever with summaries:\n",
    "\n",
    "  -   InMemoryStore stores the raw text, tables\n",
    "  -  vectorstore stores the embedded summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cf40fcb-3f99-4861-90b5-fda787093f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jyoti/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 0.4. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "# Add texts\n",
    "if texts and text_summaries:\n",
    "    doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "    summary_texts = [\n",
    "        Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "        for i, s in enumerate(text_summaries)\n",
    "    ]\n",
    "    retriever.vectorstore.add_documents(summary_texts)\n",
    "    retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
    "else:\n",
    "    print(\"No texts or text summaries to add.\")\n",
    "\n",
    "# Add tables\n",
    "if tables and table_summaries:\n",
    "    table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "    summary_tables = [\n",
    "        Document(page_content=s, metadata={id_key: table_ids[i]})\n",
    "        for i, s in enumerate(table_summaries)\n",
    "    ]\n",
    "    retriever.vectorstore.add_documents(summary_tables)\n",
    "    retriever.docstore.mset(list(zip(table_ids, tables)))\n",
    "else:\n",
    "    print(\"No tables or table summaries to add.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "860453a8-5027-410e-8078-26f8bbd935dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Prompt template\n",
    "template = \"\"\"Answer the question based only on the following context, which can include text and tables:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# LLM\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# RAG pipeline\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e485bcd-bfd1-4583-abb8-5a0cab8dc14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Gemini 1.0 model family consists of three sizes: Ultra, Pro, and Nano. \\n\\n- Ultra: This model is the most capable in the family, delivering state-of-the-art performance across a wide range of highly complex tasks, including reasoning and multimodal tasks. It is efficiently serveable at scale on TPU accelerators due to the Gemini architecture.\\n- Pro: A performance-optimized model that delivers significant performance across a wide range of tasks in terms of cost and latency. It exhibits strong reasoning performance and broad multimodal capabilities.\\n- Nano: The most efficient model designed to run on-device, with two versions - Nano-1 with 1.8B parameters and Nano-2 with 3.25B parameters, targeting low and high memory devices respectively. It is trained by distilling from larger Gemini models, 4-bit quantized for deployment, and provides best-in-class performance.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Give an overview of of the Gemini 1.0 model family\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a6f3cdf-c94b-4f4c-942c-071161c8517c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The performance of Gemini Ultra on the MMLU benchmark is an accuracy of 90.04%.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What is the performance of Gemini Ultra performance on the MMMU benchmark per discipline as per Table 8?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52d97905-5342-4093-b82a-019fefe0ee65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The provided context does not contain any information about the results of Automatic speech recognition tasks on YouTube.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What are the results of Automatic speech recognition taks on Youtube\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d21c39c-fc4c-40f8-92ae-b6f7b25fdb24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
